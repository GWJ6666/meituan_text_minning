{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prescription-livestock",
   "metadata": {},
   "source": [
    "# 美团用户评论情感分析\n",
    "\n",
    "可以看出，使用SMOTE插值与简单的数据复制比起来，AUC率略有提高，实际预测效果也挺好"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-error",
   "metadata": {},
   "source": [
    "## 1. 数据读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mounted-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "relevant-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('meituan_preprocess.xlsx')\n",
    "meituan = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bigger-scout",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\62491\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.055 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "meituan['评论用户内容_分词'] = meituan['评论用户内容'].apply(lambda x: ' '.join(jieba.cut(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coupled-custody",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>评论店铺</th>\n",
       "      <th>评论用户姓名</th>\n",
       "      <th>评论用户id</th>\n",
       "      <th>评论用户星级</th>\n",
       "      <th>评论用户菜品</th>\n",
       "      <th>评论用户内容</th>\n",
       "      <th>评论年份</th>\n",
       "      <th>评论月份</th>\n",
       "      <th>评论星期</th>\n",
       "      <th>评论小时</th>\n",
       "      <th>评论长度</th>\n",
       "      <th>评论用户内容_分词</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158743609</td>\n",
       "      <td>养了四条鱼</td>\n",
       "      <td>156107390</td>\n",
       "      <td>50</td>\n",
       "      <td>男票甜蜜双人餐，提供免费WiFi</td>\n",
       "      <td>「 # 套餐 ： 男票 甜蜜 双人 餐 」 今天 休息 ， 团 了 个 男票 的 火锅 双人...</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>113</td>\n",
       "      <td>「   #   套餐   ：   男票   甜蜜   双人   餐   」   今天   休...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158743609</td>\n",
       "      <td>养了四条鱼</td>\n",
       "      <td>156107390</td>\n",
       "      <td>50</td>\n",
       "      <td>真爱虾滑1份，包间免费</td>\n",
       "      <td>「 # 套餐 ： 真爱 虾 滑 」 好久没 吃火锅 了 ， 也 没 逛 江汉路 了 ， 嗯 ...</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>118</td>\n",
       "      <td>「   #   套餐   ：   真 爱   虾   滑   」   好久没   吃火锅  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>158743609</td>\n",
       "      <td>ddddddeft</td>\n",
       "      <td>1792254311</td>\n",
       "      <td>45</td>\n",
       "      <td>男票经典双人餐，提供免费WiFi</td>\n",
       "      <td>【 口味 】 味道 真的 不错 ， 价格 也 很 实惠 。 朋友 推荐 去 吃 的 。 今天...</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>117</td>\n",
       "      <td>【   口味   】   味道   真的   不错   ，   价格   也   很   实...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>158743609</td>\n",
       "      <td>Yoogut</td>\n",
       "      <td>1479967201</td>\n",
       "      <td>45</td>\n",
       "      <td>50元代金券1张，可叠加3张</td>\n",
       "      <td>本来 和 朋友 一起 去 吃 自助 的 ， 不 知道 怎么 莫名其妙 跑 去 女人 大 世界...</td>\n",
       "      <td>2021</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>118</td>\n",
       "      <td>本来   和   朋友   一起   去   吃   自助   的   ，   不   知道...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>158743609</td>\n",
       "      <td>浪诗的浪潮</td>\n",
       "      <td>1954780805</td>\n",
       "      <td>50</td>\n",
       "      <td>男票经典双人餐，提供免费WiFi</td>\n",
       "      <td>「 # 套餐 ： 男票 经典 双人 餐 」 七夕 出来 吃火锅 ， 团购 的 券 刚好 用 ...</td>\n",
       "      <td>2021</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>143</td>\n",
       "      <td>「   #   套餐   ：   男票   经典   双人   餐   」   七夕   出...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        评论店铺     评论用户姓名      评论用户id  评论用户星级            评论用户菜品  \\\n",
       "0  158743609      养了四条鱼   156107390      50  男票甜蜜双人餐，提供免费WiFi   \n",
       "1  158743609      养了四条鱼   156107390      50       真爱虾滑1份，包间免费   \n",
       "2  158743609  ddddddeft  1792254311      45  男票经典双人餐，提供免费WiFi   \n",
       "3  158743609     Yoogut  1479967201      45    50元代金券1张，可叠加3张   \n",
       "4  158743609      浪诗的浪潮  1954780805      50  男票经典双人餐，提供免费WiFi   \n",
       "\n",
       "                                              评论用户内容  评论年份  评论月份  评论星期  评论小时  \\\n",
       "0  「 # 套餐 ： 男票 甜蜜 双人 餐 」 今天 休息 ， 团 了 个 男票 的 火锅 双人...  2021     9     5    21   \n",
       "1  「 # 套餐 ： 真爱 虾 滑 」 好久没 吃火锅 了 ， 也 没 逛 江汉路 了 ， 嗯 ...  2021     9     5    21   \n",
       "2  【 口味 】 味道 真的 不错 ， 价格 也 很 实惠 。 朋友 推荐 去 吃 的 。 今天...  2021     9     4    10   \n",
       "3  本来 和 朋友 一起 去 吃 自助 的 ， 不 知道 怎么 莫名其妙 跑 去 女人 大 世界...  2021     8     6    23   \n",
       "4  「 # 套餐 ： 男票 经典 双人 餐 」 七夕 出来 吃火锅 ， 团购 的 券 刚好 用 ...  2021     8     1    18   \n",
       "\n",
       "   评论长度                                          评论用户内容_分词  \n",
       "0   113  「   #   套餐   ：   男票   甜蜜   双人   餐   」   今天   休...  \n",
       "1   118  「   #   套餐   ：   真 爱   虾   滑   」   好久没   吃火锅  ...  \n",
       "2   117  【   口味   】   味道   真的   不错   ，   价格   也   很   实...  \n",
       "3   118  本来   和   朋友   一起   去   吃   自助   的   ，   不   知道...  \n",
       "4   143  「   #   套餐   ：   男票   经典   双人   餐   」   七夕   出...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meituan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-college",
   "metadata": {},
   "source": [
    "## 2. 构建标签值\n",
    "\n",
    "美团用户评论的评论星级为 0~5 星级，用 **评论用户星级** 这一属性代表，它的取值范围为 5-50，每 5 个单位代表 半颗星，例如 取值为 5 代表 0.5 星，取值 45 代表 4.5 星。\n",
    "\n",
    "因此，我们将 3 星以上的取标签为 好评，3 星以及以下的为 差评，好评用 1 代表， 差评用 0 代表，我们的情感评分可以转化为标签值为 1 的概率值，这样我们就把情感分析问题转为文本分类问题了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "starting-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(score):\n",
    "    if score > 30:\n",
    "        return 1\n",
    "    elif score <= 30:\n",
    "        return 0\n",
    "    \n",
    "#标签转化\n",
    "meituan['分类得分'] = meituan['评论用户星级'].map(lambda x: label(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-antibody",
   "metadata": {},
   "source": [
    "## 3. 文本特征处理\n",
    "\n",
    "中文文本特征处理，需要进行中文分词，分词之后需要过滤停用词，也就会去除无效的语气词之类的，最后要进行文本转向量，有词库表示法、TF-IDF、word2vec等。\n",
    "\n",
    "首先对数据进行分割为测试集与训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pregnant-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "##切分测试集、训练集\n",
    "x_train, x_test, y_train, y_test = train_test_split(meituan['评论用户内容'], meituan['分类得分'], random_state=3, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-casino",
   "metadata": {},
   "source": [
    "接着引入停用词，去除文本中无效词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "spanish-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"stopwords.txt\",encoding='utf-8')\n",
    "stopwords_lst = infile.readlines()\n",
    "stopwords = [x.strip() for x in stopwords_lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-synthesis",
   "metadata": {},
   "source": [
    "接着进行中文分词处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "helpful-dubai",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13654     菜品   品种   比较   齐全   ，   口味   还   行   。   价格   稍贵\n",
       "16311    「   #   套餐   ：   2   -   3   人网   红   火锅   套餐 ...\n",
       "15304    价格   实惠   ，   口味   不错   ，   两个   人   团购   的   ...\n",
       "10413    挺不错   的   ，   ，   口味   挺   好   ！   就是   每串   太...\n",
       "10666    味道   不错   ，   就   在   家门口   ，   家   楼下   ，   好...\n",
       "Name: 评论用户内容, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fenci(train_data):\n",
    "    words_df = train_data.apply(lambda x:' '.join(jieba.cut(x)))\n",
    "    return words_df\n",
    "\n",
    "x_train_fenci = fenci(x_train)\n",
    "x_train_fenci[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-driving",
   "metadata": {},
   "source": [
    "最后就是文本特征提取\n",
    "\n",
    "CountVectorizer旨在通过计数来将一个文档转换为向量。当不存在先验字典时，Countvectorizer作为Estimator提取词汇进行训练，并生成一个CountVectorizerModel用于存储相应的词汇向量空间。该模型产生文档关于词语的稀疏表示。下面举一个例子示范："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "serial-values",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 4, 0, 1, 0],\n",
       "        [1, 0, 1, 1, 0],\n",
       "        [0, 0, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用于转词向量的语料\n",
    "yuliao = ['dog cat fish dog dog dog','cat eat fish','i like eat fish']\n",
    "\n",
    "#sklearn库CountVectorizer转词向量\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "vector = cv.fit_transform(yuliao)\n",
    "vector.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "civilian-raising",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog': 1, 'cat': 0, 'fish': 3, 'eat': 2, 'like': 4}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "auburn-daniel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'lex', 'll', 'mon', 'null', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=3000, ngram_range=(1, 2),\n",
       "                stop_words=['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*',\n",
       "                            '+', ',', '-', '--', '.', '..', '...', '......',\n",
       "                            '...................', './', '.一', '记者', '数', '年',\n",
       "                            '月', '日', '时', '分', '秒', '/', ...])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用TF-IDF进行文本转向量处理\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(stop_words=stopwords, max_features=3000, ngram_range=(1,2))\n",
    "tv.fit(x_train_fenci)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-victorian",
   "metadata": {},
   "source": [
    "### 机器学习建模\n",
    "\n",
    "特征和标签已经准备好了，接下来就是建模了。这里我们使用文本分类的经典算法朴素贝叶斯算法，而且朴素贝叶斯算法的计算量较少。特征值是评论文本经过TF-IDF处理的向量，标签值评论的分类共两类，好评是1，差评是0。情感评分为分类器预测分类1的概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "planned-trading",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8214616096207216"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算分类效果的准确率\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(tv.transform(x_train), y_train)\n",
    "classifier.score(tv.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "superb-department",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8110449531563964"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算分类器的AUC值\n",
    "y_pred = classifier.predict_proba(tv.transform(x_test))[:,1]\n",
    "roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "relevant-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算一条评论文本的情感评分\n",
    "def ceshi(model,strings):\n",
    "    strings_fenci = fenci(pd.Series([strings]))\n",
    "    return float(model.predict_proba(tv.transform(strings_fenci))[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "composite-legislature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好评实例的模型预测情感得分为0.6808873012465957\n",
      "差评实例的模型预测情感得分为0.42542752977565185\n"
     ]
    }
   ],
   "source": [
    "#从美团网找两条评论来测试一下\n",
    "test1 = '菜品种类超级多，熟食很多，水果饮料鲜榨种类很丰富，环境也蛮好的，就是进去的时候门不大好找！肉类品种很丰富，还有最爱的小龙虾，就是刚上五分钟就抢没了～盘子收的很勤快，服务态度也好，还有工作人员让我提意见，哈哈，我这个憨憨直说水果不甜...' #5星好评\n",
    "test2 = '这不是欺骗顾客吗，根本没有龙虾，更气的是每个服务员都说有，让人白等！ ！欺骗顾客！没有就应该早点讲！ ！' #1星差评\n",
    "print('好评实例的模型预测情感得分为{}\\n差评实例的模型预测情感得分为{}'.format(ceshi(classifier,test1),ceshi(classifier,test2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-cargo",
   "metadata": {},
   "source": [
    "可以看出，准确率和AUC值都非常不错的样子，但点评网上的实际测试中，5星好评模型预测出来了，1星差评缺预测错误。为什么呢？我们查看一下混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "effective-skirt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 467,  784],\n",
       "       [ 181, 3973]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_predict = classifier.predict(tv.transform(x_test))\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-stanley",
   "metadata": {},
   "source": [
    "可以看出，负类的预测非常不准，433单准确预测为负类的只有15.7%，应该是由于数据不平衡导致的，模型的默认阈值为输出值的中位数。比如逻辑回归的输出范围为[0,1]，当某个样本的输出大于0.5就会被划分为正例，反之为反例。在数据的类别不平衡时，采用默认的分类阈值可能会导致输出全部为正例，产生虚假的高准确度，导致分类失败。\n",
    "\n",
    "处理样本不均衡问题的方法，首先可以选择调整阈值，使得模型对于较少的类别更为敏感，或者选择合适的评估标准，比如ROC或者F1，而不是准确度（accuracy）。另外一种方法就是通过采样（sampling）来调整数据的不平衡。其中欠采样抛弃了大部分正例数据，从而弱化了其影响，可能会造成偏差很大的模型，同时，数据总是宝贵的，抛弃数据是很奢侈的。另外一种是过采样，下面我们就使用过采样方法来调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-turkey",
   "metadata": {},
   "source": [
    "### 过采样\n",
    "单纯的重复了反例，因此会过分强调已有的反例。如果其中部分点标记错误或者是噪音，那么错误也容易被成倍的放大。因此最大的风险就是对反例过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "closing-activation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    16510\n",
       "0     5108\n",
       "Name: 分类得分, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meituan['分类得分'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "experienced-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把0类样本复制10次，构造训练集\n",
    "index_tmp = y_train==0\n",
    "y_tmp = y_train[index_tmp]\n",
    "x_tmp = x_train[index_tmp]\n",
    "x_train2 = pd.concat([x_train,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp])\n",
    "y_train2 = pd.concat([y_train,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-passion",
   "metadata": {},
   "source": [
    "#使用过采样样本(简单复制)进行模型训练，并查看准确率\n",
    "clf2 = MultinomialNB()\n",
    "clf2.fit(tv.transform(x_train2), y_train2)\n",
    "y_pred2 = clf2.predict_proba(tv.transform(x_test))[:,1]\n",
    "roc_auc_score(y_test,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "italic-modification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1157,   94],\n",
       "       [2486, 1668]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看此时的混淆矩阵\n",
    "y_predict2 = clf2.predict(tv.transform(x_test))\n",
    "cm = confusion_matrix(y_test, y_predict2)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-associate",
   "metadata": {},
   "source": [
    "可以看出，即使是简单粗暴的复制样本来处理样本不平衡问题，负样本的识别率大幅上升了，变为77%，满满的幸福感呀~我们自己写两句评语来看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "incomplete-flower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015253417587460566"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ceshi(clf2,'排队人太多，环境不好，口味一般')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-substitute",
   "metadata": {},
   "source": [
    "可以看出把0类别的识别出来了，太棒了~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-attribute",
   "metadata": {},
   "source": [
    "## 过采样（SMOTE算法）\n",
    "SMOTE（Synthetic minoritye over-sampling technique,SMOTE），是在局部区域通过K-近邻生成了新的反例。相较于简单的过采样，SMOTE降低了过拟合风险，但同时运算开销加大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "addressed-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用SMOTE进行样本过采样处理\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversampler= SMOTE(random_state=42)\n",
    "x_train_vec = tv.transform(x_train)\n",
    "x_resampled, y_resampled = oversampler.fit_resample(x_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "joined-oakland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12356\n",
       "0     3857\n",
       "Name: 分类得分, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#原始的样本分布\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "asian-gibson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12356\n",
       "1    12356\n",
       "Name: 分类得分, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#经过SMOTE算法过采样后的样本分布情况\n",
    "pd.Series(y_resampled).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-student",
   "metadata": {},
   "source": [
    "\n",
    "我们经过插值，把0类数据也丰富为 12356 个数据了，这时候正负样本的比例为1:1，接下来我们用平衡后的数据进行训练，效果如何呢，好期待啊~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "electoral-bread",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8152047452072044"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用过采样样本(SMOTE)进行模型训练，并查看准确率\n",
    "clf3 = MultinomialNB()\n",
    "clf3.fit(x_resampled, y_resampled)\n",
    "y_pred3 = clf3.predict_proba(tv.transform(x_test))[:,1]\n",
    "roc_auc_score(y_test,y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "express-reduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 882,  369],\n",
       "       [1050, 3104]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看此时的准确率\n",
    "y_predict3 = clf3.predict(tv.transform(x_test))\n",
    "cm = confusion_matrix(y_test, y_predict3)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "herbal-trace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25750452237572696"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#到网上找一条差评来测试一下情感评分的预测效果\n",
    "test3 = '这不是欺骗顾客吗，根本没有龙虾，更气的是每个服务员都说有，让人白等！ ！欺骗顾客！没有就应该早点讲！ ！'\n",
    "ceshi(clf3,test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-accused",
   "metadata": {},
   "source": [
    "可以看出，使用SMOTE插值与简单的数据复制比起来，AUC率略有提高，实际预测效果也挺好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-climb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aging-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "#词向量训练\n",
    "tv2 = TfidfVectorizer(stop_words=stopwords, max_features=3000, ngram_range=(1,2))\n",
    "tv2.fit(meituan['评论用户内容'])\n",
    "\n",
    "#SMOTE插值\n",
    "X_tmp = tv2.transform(meituan['评论用户内容'])\n",
    "y_tmp = meituan['分类得分']\n",
    "sm = SMOTE(random_state=0)\n",
    "X,y = sm.fit_resample(X_tmp, y_tmp)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "def fenxi(strings):\n",
    "    strings_fenci = fenci(pd.Series([strings]))\n",
    "    return float(clf.predict_proba(tv2.transform(strings_fenci))[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "tribal-platinum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06961621359910641"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fenxi('不明白那么多五星好评是怎么来的，价格贵你好吃就算了，重点是贵还难吃，两个人吃了300多，麻辣锅底不麻不辣，一股中药味，番茄锅没有蕃茄味，只有酸酸的还不是番茄的味道，69的肥牛就10片，网红企鹅真的巨难吃，除了跳水啥也不是，唯一能过得去的只有去骨鸡肉，莴笋还可以，就是特别大的一块，都不知道怎么下口吃，跑这么远吃的一点都不开心，买单的时候也没有服务员，居然有人说服务比海底捞好？？？请的海底捞的黑粉吗？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-journalist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
